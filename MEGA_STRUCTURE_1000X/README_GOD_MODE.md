# ğŸš€ CLAWZENEGER GOD MODE - Stack Completo

## ğŸ“Œ Â¿QuÃ© es esto?

**Clawzeneger God Mode** es un ecosistema completo de IA con **12 servicios integrados** en Docker que te permite:

- ğŸ§  **Ejecutar LLMs locales** (Ollama) y en la nube (Hugging Face) con la misma API
- ğŸ“± **Automatizar WhatsApp** con respuestas de IA
- âš¡ **Crear workflows complejos** con n8n
- ğŸ“š **Memoria infinita** con ChromaDB (Vector DB)
- ğŸ—£ï¸ **Voces realistas** con XTTS (Text-to-Speech)
- ğŸ‘‚ **TranscripciÃ³n de audio** con Whisper (Speech-to-Text)
- ğŸ” **BÃºsqueda privada** con SearXNG
- ğŸ‘ï¸ **Web scraping** con Browserless

---

## ğŸ¯ Quick Start (5 minutos)

### 1ï¸âƒ£ Pre-requisitos

- âœ… Windows 11 con WSL2
- âœ… Docker Desktop instalado y corriendo
- âœ… MÃ­nimo 16GB RAM, 50GB espacio en disco

### 2ï¸âƒ£ ConfiguraciÃ³n

```powershell
cd c:\CLAWZENEGER\MEGA_STRUCTURE_1000X

# Copia el .env de ejemplo
copy .env.example .env

# Edita con tus credenciales reales
notepad .env
```

**IMPORTANTE:** Necesitas obtener:
- `HF_TOKEN` de https://huggingface.co/settings/tokens
- `TELEGRAM_BOT_TOKEN` de @BotFather en Telegram (opcional)
- `WHATSAPP_API_KEY` (genera uno seguro)

### 3ï¸âƒ£ Lanzar el Stack

```powershell
# OpciÃ³n fÃ¡cil: Usar el script
.\LAUNCH_GOD_MODE_FINAL.ps1

# OpciÃ³n manual
docker-compose -f docker-compose.god_mode.FINAL.yml up -d
```

### 4ï¸âƒ£ Verificar

```powershell
# Ejecutar script de verificaciÃ³n
.\TEST_GOD_MODE.ps1

# DeberÃ­as ver 12 servicios con âœ…
```

---

## ğŸŒ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLAWZENEGER ECOSYSTEM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Agente 1 â”‚  â”‚ Agente 2 â”‚  â”‚   n8n    â”‚  â”‚    UI    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚             â”‚             â”‚             â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                         â”‚                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚     HF-PROXY        â”‚ â—„â”€â”€ AquÃ­ estÃ¡ la magia     â”‚
â”‚              â”‚    (LiteLLM)        â”‚                            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â”‚                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚         â”‚               â”‚               â”‚                       â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚ Ollama  â”‚    â”‚ HuggingFâ”‚    â”‚ OpenAI  â”‚ (opcional)        â”‚
â”‚    â”‚ (Local) â”‚    â”‚ (Cloud) â”‚    â”‚ (Cloud) â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas:**
- âœ… Todos los componentes hablan el mismo "idioma" (OpenAI API)
- âœ… CachÃ© automÃ¡tico con Redis (ahorra tokens y tiempo)
- âœ… Failover: Si falla Hugging Face, usa Ollama local
- âœ… Sin modificar cÃ³digo de agentes existentes

---

## ğŸ“¦ Servicios Incluidos

| Servicio | Puerto | FunciÃ³n | Estado |
|----------|--------|---------|--------|
| **Ollama** | 11434 | LLM local (GPU) | âœ… Core |
| **HF-Proxy** | 4000 | Proxy unificado LLM | âœ… Core |
| **OpenWebUI** | 3000 | Interfaz chat | âœ… Core |
| **n8n** | 5678 | AutomatizaciÃ³n | âœ… Core |
| **ChromaDB** | 8000 | Vector DB (memoria) | âœ… Core |
| **Redis** | 6379 | Cache | âœ… Core |
| **PostgreSQL** | 5432 | DB analytics | âœ… Core |
| **WhatsApp** | 8080 | Gateway Evolution API | âš¡ Addon |
| **SearXNG** | 8081 | BÃºsqueda privada | âš¡ Addon |
| **Whisper** | 9000 | Speech-to-Text | âš¡ Addon |
| **XTTS** | 5002 | Text-to-Speech | âš¡ Addon |
| **Browserless** | 3001 | Web scraping | âš¡ Addon |

---

## ğŸ§  HF-Proxy: El CorazÃ³n del Sistema

### Â¿QuÃ© hace?

El **HF-Proxy** (powered by LiteLLM) es un proxy que:
1. Recibe peticiones en formato OpenAI
2. Las traduce al formato correcto para cada proveedor (HuggingFace, Ollama, etc.)
3. Cachea respuestas para ahorrar tokens
4. Maneja failover automÃ¡tico si un modelo falla

### Modelos Disponibles

```yaml
# Desde la configuraciÃ³n en config/litellm/config.yaml

Modelos HuggingFace (Cloud):
- llama-3.2-3b          # RÃ¡pido, conversacional
- deepseek-r1-7b        # Razonamiento avanzado
- qwen-2.5-7b           # Especializado en cÃ³digo

Modelos Ollama (Local):
- nexobot-he            # Tu modelo personalizado
- llama3-local          # Llama 3.2 local
```

### Ejemplo de Uso

**Desde Python:**
```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-clawzeneger-master-2026",
    base_url="http://localhost:4000/v1"
)

response = client.chat.completions.create(
    model="llama-3.2-3b",
    messages=[{"role": "user", "content": "Hola"}]
)

print(response.choices[0].message.content)
```

**Desde n8n:**
```http
POST http://hf-proxy:8000/v1/chat/completions
Content-Type: application/json
Authorization: Bearer sk-clawzeneger-master-2026

{
  "model": "llama-3.2-3b",
  "messages": [{"role": "user", "content": "Hola"}]
}
```

---

## âš¡ Workflows n8n Incluidos

### 1. WhatsApp AI Responder
**Archivo:** `workflows_n8n/whatsapp_ai_responder.json`

**FunciÃ³n:** Responder automÃ¡ticamente a mensajes de WhatsApp usando IA

**Flow:**
1. Recibe mensaje de WhatsApp vÃ­a webhook
2. Filtra mensajes propios (no responder a uno mismo)
3. EnvÃ­a a HF-Proxy para generar respuesta
4. Responde por WhatsApp
5. Guarda conversaciÃ³n en ChromaDB para memoria

### 2. Telegram Lead Hunter
**Archivo:** `workflows_n8n/telegram_hunter.json`

**FunciÃ³n:** Detectar leads potenciales en grupos de Telegram

**Flow:**
1. Escucha mensajes de Telegram
2. Analiza con IA si es un lead potencial
3. Si es lead, envÃ­a notificaciÃ³n a admin

### 3. Payment Generator
**Archivo:** `workflows_n8n/payment_generator.json`

**FunciÃ³n:** Generar links de pago automÃ¡ticamente

**Flow:**
1. Recibe peticiÃ³n de pago
2. Genera link de MercadoPago/PayPal/SPEI
3. Retorna link para enviar al cliente

---

## ğŸ”§ Comandos Ãštiles

### GestiÃ³n del Stack

```powershell
# Lanzar todo
.\LAUNCH_GOD_MODE_FINAL.ps1

# Lanzar con limpieza completa
.\LAUNCH_GOD_MODE_FINAL.ps1 -Clean

# Ver logs en tiempo real
docker-compose -f docker-compose.god_mode.FINAL.yml logs -f

# Ver logs de un servicio especÃ­fico
docker logs -f claw-brain-hfproxy

# Detener todo
docker-compose -f docker-compose.god_mode.FINAL.yml down

# Detener y eliminar volÃºmenes (CUIDADO: borra datos)
docker-compose -f docker-compose.god_mode.FINAL.yml down -v

# Reiniciar un servicio
docker restart claw-whatsapp-evolution

# Verificar salud
.\TEST_GOD_MODE.ps1
```

### Testing

```powershell
# Test del HF-Proxy
curl http://localhost:4000/health

# Test de modelo especÃ­fico
$body = @{
    model = "llama-3.2-3b"
    messages = @(@{role="user"; content="Test"})
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:4000/v1/chat/completions" `
    -Method Post `
    -Headers @{"Authorization"="Bearer sk-clawzeneger-master-2026"} `
    -Body $body
```

---

## ğŸ“Š Monitoreo

### Ver mÃ©tricas de LiteLLM
http://localhost:4000/metrics

### Ver logs de n8n
http://localhost:5678/executions

### Ver memoria de ChromaDB
```powershell
curl http://localhost:8000/api/v1/collections
```

---

## ğŸš¨ Troubleshooting

### Problema: "Docker daemon not running"
**SoluciÃ³n:** Abre Docker Desktop

### Problema: "Port 3000 already in use"
**SoluciÃ³n:** 
```powershell
netstat -ano | findstr :3000
taskkill /PID <PID> /F
```

### Problema: "HF-Proxy error 401"
**SoluciÃ³n:** Verifica `HF_TOKEN` en `.env`

### Problema: Servicios no inician
**SoluciÃ³n:**
```powershell
# Ver logs
docker-compose -f docker-compose.god_mode.FINAL.yml logs

# Reiniciar desde cero
docker-compose -f docker-compose.god_mode.FINAL.yml down -v
.\LAUNCH_GOD_MODE_FINAL.ps1 -Clean
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- **GuÃ­a completa:** `IMPLEMENTATION_PLAN_FINAL.md`
- **Ejemplos de cÃ³digo:** `examples/hf_proxy_examples.py`
- **Config de LiteLLM:** `config/litellm/config.yaml`

---

## ğŸ¯ Roadmap

### âœ… Completado
- [x] Stack Docker completo con 12 servicios
- [x] HF-Proxy integrado con cachÃ©
- [x] 3 workflows de n8n funcionales
- [x] WhatsApp gateway configurado
- [x] Voces XTTS integradas

### ğŸš§ En Progreso
- [ ] Conectar UI custom (CLAWZENEGER-UI)
- [ ] Crear agentes especializados (CEO, DEV, SPY)
- [ ] Training de voz personalizada

### ğŸ“‹ Planeado
- [ ] Setup de cluster de 2 PCs
- [ ] Migrar a cloud (Oracle/AWS)
- [ ] Implementar sales pipeline completo

---

## ğŸ“ Soporte

Para reportar issues o contribuir:
- GitHub: (tu repo si lo publicas)
- DocumentaciÃ³n tÃ©cnica: Ver carpeta `MEGA_STRUCTURE_1000X/`

---

**Built with ğŸ’ª by NeoWolf/Roberto**  
**Project:** CLAWZENEGER 10X  
**Date:** Feb 2026
